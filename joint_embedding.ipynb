{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0522613",
   "metadata": {},
   "source": [
    "# Joint Embedding of Protein Signals and Amino Acid Sequences\n",
    "\n",
    "This notebook aims to create joint embeddings of protein signals and their corresponding amino acid sequences in the same vector space. \n",
    "\n",
    "TODO:\n",
    "- Dataset\n",
    "    - Make the nanopore readings a consistent size so an input dem can be \n",
    "- Model\n",
    "    - Save model weights on completition\n",
    "- Evaluation\n",
    "    - Crete a dummy classifier (not sure how)\n",
    "    - Compare to the CNN and RF models\n",
    "    - Try to do a 1:1 comparison between existing models and my embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcbd4b9",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f07b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nanopore_readings_path = './data/run_df.json'\n",
    "EMBEDDING_DIM = 64\n",
    "HIDDEN_DIM = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc3544",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e618b663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os, time\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import json\n",
    "# import math\n",
    "# import warnings\n",
    "# import scipy.signal as scisignal\n",
    "# import sklearn.utils.class_weight as class_weight\n",
    "# from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ca5d3",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0173ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A - Alanine\n",
    "R - Arginine\n",
    "N - Asparagine\n",
    "D - Aspartic Acid\n",
    "C - Cysteine\n",
    "E - Glutamic Acid\n",
    "Q - Glutamine\n",
    "G - Glycine\n",
    "H - Histidine\n",
    "I - Isoleucine\n",
    "L - Leucine\n",
    "K - Lysine\n",
    "M - Methionine\n",
    "F - Phenylalanine\n",
    "P - Proline\n",
    "S - Serine\n",
    "T - Threonine\n",
    "W - Tryptophan\n",
    "Y - Tyrosine\n",
    "V - Valine\n",
    "\"\"\"\n",
    "# I would prefer these to be sorted, but I am maintaining the order from the original code.\n",
    "amino_acids_indexes = {aa:i for i, aa in enumerate('CSAGTVNQMILYWFPHRKDE')}\n",
    "\n",
    "run_to_peptide = {\n",
    "    '20220824_run02_a': 'HDKER',\n",
    "    '20220826_run01_a': 'GNQST',\n",
    "    '20220826_run02_a': 'FYWCP',\n",
    "    '20220826_run03_a': 'AVLIM',\n",
    "    '20220907_run01_a': 'GNQST',\n",
    "    '20221010_run02_a': 'GNQST',\n",
    "    '20221011_run02_a': 'HDKER',\n",
    "    '20221026_run01_a': 'VGDNY',\n",
    "    '20221028_run01_a': 'TWAFH',\n",
    "    '20221028_run02_a': 'PRMQE',\n",
    "    '20221107_run01_a': 'TWAFH',\n",
    "    '20221108_run01_a': 'TWAFH',\n",
    "    '20221109_run01_a': 'VGDNY',\n",
    "    '20221109_run02_a': 'PRMQE',\n",
    "    '20221109_run03_a': 'KSILC',\n",
    "    '20221109_run04_a': 'FYWCP',\n",
    "    '20221110_run02_a': 'AVLIM',\n",
    "    '20221121_run01_a': 'KSILC',\n",
    "    '20221122_run01_a': 'KSILC',\n",
    "    '20221122_run02_a': 'PRMQE',\n",
    "    '20221122_run03_a': 'KSILC', \n",
    "    '20221213_run02_a': 'FYWCP',\n",
    "    '20221214_run01_a': 'FYWCP',\n",
    "    '20221214_run04_a': 'FYWCP',\n",
    "    '20221219_run01_a': 'VGDNY',\n",
    "    '20221220_run01_a': 'FYWCP',\n",
    "    '20221220_run04_a': 'AVLIM',\n",
    "    '20221221_run01_a': 'TWAFH',\n",
    "    '20221221_run02_a': 'KSILC',\n",
    " }\n",
    "\n",
    "vocab_size = len(amino_acids_indexes)\n",
    "\n",
    "def path_to_dataframe(nanopore_readings_path):\n",
    "    \"\"\"\n",
    "    Converts the nanopore readings JSON file to a pandas DataFrame with readings as numpy arrays.\n",
    "    \"\"\"\n",
    "    with open(nanopore_readings_path, 'r') as f:\n",
    "        loaded_json = json.load(f)\n",
    "    data_dict = loaded_json['data']\n",
    "    records = [\n",
    "        {'run_id': run_id, \n",
    "         'readings': np.array(list(readings_dict.values())[0], dtype=np.float32), \n",
    "         'peptide': run_to_peptide[run_id]}\n",
    "        for run_id, readings_dict in data_dict.items()\n",
    "    ]\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "nanopore_df = path_to_dataframe(nanopore_readings_path)\n",
    "\n",
    "class PastorDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset nanopore readings and amino acid sequences.\n",
    "    \"\"\"\n",
    "    def __init__(self, nanopore_df, amino_acid_tokenizer):\n",
    "        self.nanopore_df = nanopore_df\n",
    "        self.amino_acid_tokenizer = amino_acid_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nanopore_df)\n",
    "\n",
    "    \"\"\"\n",
    "    Need to revisit this\n",
    "    \"\"\"\n",
    "    def __getitem__(self, idx): \n",
    "        nanopore_sample = torch.tensor(self.nanopore_df.iloc[idx].values, dtype=torch.float32)\n",
    "\n",
    "        # Positive amino acid sample\n",
    "        positive_amino_acid = self.nanopore_df[idx].peptide\n",
    "        positive_amino_acid_tokenized = self.amino_acid_tokenizer(positive_amino_acid)\n",
    "\n",
    "        # Negative amino acid sample\n",
    "        negative_idx = np.random.randint(0, len(self.nanopore_df))\n",
    "        while negative_idx == idx or self.nanopore_df.iloc[negative_idx].peptide == positive_amino_acid:\n",
    "            negative_idx = np.random.randint(0, len(self.amino_acids))\n",
    "        negative_amino_acid = self.nanopore_df[negative_idx]\n",
    "        negative_amino_acid_tokenized = self.amino_acid_tokenizer(negative_amino_acid)\n",
    "\n",
    "        return (\n",
    "            nanopore_sample,\n",
    "            positive_amino_acid_tokenized,\n",
    "            negative_amino_acid_tokenized\n",
    "        )\n",
    "\n",
    "# def amino_acid_tokenizer(sequence, char_to_int, max_length=100):\n",
    "#     \"\"\"\n",
    "#     Tokenizes an amino acid sequence.\n",
    "#     \"\"\"\n",
    "#     encoded = [char_to_int[char] for char in sequence]\n",
    "#     padded = np.zeros(max_length, dtype=np.int64)\n",
    "#     length = len(encoded)\n",
    "#     if length > max_length:\n",
    "#         length = max_length\n",
    "#     padded[:length] = encoded[:length]\n",
    "#     return torch.tensor(padded)\n",
    "\n",
    "dataset = PastorDataset(nanopore_df, lambda x: torch.tensor([amino_acids_indexes[aa] for aa in x], dtype=torch.long))\n",
    "data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70cbe19",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed618d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NanoporeReadingEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder for the run_df data. It's a simple Multi-Layer Perceptron (MLP).\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(NanoporeReadingEncoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class PastorEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder for the PASTOR amino acid sequences. It uses a Bidirectional LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(PastorEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        # We use the concatenation of the final forward and backward hidden states\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "class JointEmbeddingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The main model that combines the two encoders.\n",
    "    \"\"\"\n",
    "    def __init__(self, run_df_encoder, amino_acid_encoder):\n",
    "        super(JointEmbeddingModel, self).__init__()\n",
    "        self.run_df_encoder = run_df_encoder\n",
    "        self.amino_acid_encoder = amino_acid_encoder\n",
    "\n",
    "    def forward(self, run_df, amino_acid):\n",
    "        run_df_embedding = self.run_df_encoder(run_df)\n",
    "        amino_acid_embedding = self.amino_acid_encoder(amino_acid)\n",
    "        return run_df_embedding, amino_acid_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6e4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2, keepdim = True)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive\n",
    "\n",
    "def train_model(model, data_loader, epochs=10, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Training loop for the joint embedding model.\n",
    "    \"\"\"\n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (run_df_sample, pos_amino_acid, neg_amino_acid) in enumerate(data_loader):\n",
    "            run_df_sample = run_df_sample.to(device)\n",
    "            pos_amino_acid = pos_amino_acid.to(device)\n",
    "            neg_amino_acid = neg_amino_acid.to(device)\n",
    "\n",
    "            # Positive pair\n",
    "            optimizer.zero_grad()\n",
    "            run_df_embedding, pos_amino_acid_embedding = model(run_df_sample, pos_amino_acid)\n",
    "            loss_pos = criterion(run_df_embedding, pos_amino_acid_embedding, torch.zeros(run_df_sample.size(0), 1).to(device))\n",
    "\n",
    "            # Negative pair\n",
    "            run_df_embedding, neg_amino_acid_embedding = model(run_df_sample, neg_amino_acid)\n",
    "            loss_neg = criterion(run_df_embedding, neg_amino_acid_embedding, torch.ones(run_df_sample.size(0), 1).to(device))\n",
    "\n",
    "            loss = loss_pos + loss_neg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce9cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, top_k=5):\n",
    "    \"\"\"\n",
    "    Evaluates the model by checking the retrieval accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for run_df_sample, pos_amino_acid, _ in data_loader:\n",
    "            run_df_sample = run_df_sample.to(device)\n",
    "            pos_amino_acid = pos_amino_acid.to(device)\n",
    "\n",
    "            run_df_embedding, pos_amino_acid_embedding = model(run_df_sample, pos_amino_acid)\n",
    "\n",
    "            # Create a batch of negative samples for evaluation\n",
    "            # (In a real scenario, you'd use a larger set of candidates)\n",
    "            batch_size = run_df_sample.size(0)\n",
    "            candidate_indices = np.random.choice(len(data_loader.dataset), size=batch_size, replace=False)\n",
    "            candidate_amino_acids = [data_loader.dataset.amino_acids[i] for i in candidate_indices]\n",
    "            candidate_amino_acids_tokenized = torch.stack(\n",
    "                [data_loader.dataset.amino_acid_tokenizer(seq) for seq in candidate_amino_acids]\n",
    "            ).to(device)\n",
    "            _, candidate_embeddings = model(run_df_sample, candidate_amino_acids_tokenized)\n",
    "\n",
    "            # Calculate distances\n",
    "            distances = torch.cdist(run_df_embedding, torch.cat([pos_amino_acid_embedding, candidate_embeddings]))\n",
    "\n",
    "            # Check if the positive sample is in the top_k closest\n",
    "            _, top_k_indices = torch.topk(distances, top_k, largest=False)\n",
    "            correct += (top_k_indices == 0).any(dim=1).sum().item()\n",
    "            total += batch_size\n",
    "\n",
    "    print(f\"Top-{top_k} Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bcce62",
   "metadata": {},
   "source": [
    "## Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc14bf11",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'run_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 4. Model Initialization ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m run_df_encoder = NanoporeReadingEncoder(input_dim=\u001b[43mrun_df\u001b[49m.shape[\u001b[32m1\u001b[39m], embedding_dim=EMBEDDING_DIM)\n\u001b[32m      3\u001b[39m amino_acid_encoder = PastorEncoder(vocab_size=vocab_size, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n\u001b[32m      4\u001b[39m model = JointEmbeddingModel(run_df_encoder, amino_acid_encoder)\n",
      "\u001b[31mNameError\u001b[39m: name 'run_df' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 4. Model Initialization ---\n",
    "run_df_encoder = NanoporeReadingEncoder(input_dim=nanopore_df, embedding_dim=EMBEDDING_DIM)\n",
    "amino_acid_encoder = PastorEncoder(vocab_size=vocab_size, embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM)\n",
    "model = JointEmbeddingModel(run_df_encoder, amino_acid_encoder)\n",
    "\n",
    "\n",
    "# --- 5. Training ---\n",
    "print(\"--- Training ---\")\n",
    "trained_model = train_model(model, data_loader, epochs=EPOCHS)\n",
    "\n",
    "\n",
    "# --- 6. Evaluation ---\n",
    "print(\"\\n--- Evaluation ---\")\n",
    "evaluate_model(trained_model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f767d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = path_to_dataframe(nanopore_readings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5618cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14023"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ex.readings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9908f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to normalize the length of the readings for a consistent input size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pastor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
